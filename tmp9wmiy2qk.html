<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Exceptions Report</title>
    <style>
    body {
    font-family: Arial, sans-serif;
    line-height: 1.6;
    background-color: #f9f9f9;
    color: #333;
    margin: 20px;
}

.interview {
    font-size: 1.5em;
    margin-bottom: 10px;
    padding: 10px;
    background-color: #e3f2fd;
    border-left: 5px solid #2196f3;
}

.question {
    font-size: 1.2em;
    margin-bottom: 10px;
    padding: 10px;
    background-color: #fff9c4;
    border-left: 5px solid #ffeb3b;
}

.exception-detail {
    margin-bottom: 10px;
    background-color: #ffebee;
    border-left: 5px solid #f44336;
}

.exception-header {
    padding: 10px;
    cursor: pointer;
    display: flex;
    justify-content: space-between;
    align-items: center;
}

.exception-content {
    padding: 10px;
    display: none;
}

.exception-content.show {
    display: block;
}

.question-detail {
    border: 3px solid black;
    padding: 10px;
}

.exception-exception {
    font-weight: bold;
    color: #d32f2f;
}

.exception-time,
.exception-traceback {
    font-style: italic;
    color: #555;
}

.toggle-btn {
    background: none;
    border: none;
    font-size: 1.2em;
    cursor: pointer;
    transition: transform 0.3s;
}

.toggle-btn.rotated {
    transform: rotate(180deg);
}
    </style>

    <script>
    document.addEventListener('DOMContentLoaded', function() {
    const collapsibleSections = document.querySelectorAll('.exception-detail, .raw-model-response');

    collapsibleSections.forEach(section => {
        const header = section.querySelector('.exception-header, .response-header');
        const content = section.querySelector('.exception-content, .response-content');
        const toggleBtn = section.querySelector('.toggle-btn');

        header.addEventListener('click', function() {
            content.classList.toggle('show');
            toggleBtn.classList.toggle('rotated');
        });
    });

});

function copyCode() {
    const textarea = document.getElementById('codeToCopy');
    textarea.select();
    textarea.setSelectionRange(0, 99999); // For mobile devices
    document.execCommand("copy");

    // Optionally, you can display an alert or change the button text to indicate success
    alert("Code copied to clipboard!");
}

    </script>

</head>
<body>
    <style>
    td {
        padding: 0 10px; /* This applies the padding uniformly to all td elements */
    }
</style>

<h1>Exceptions Report</h1>
<p>
    This report summarizes exceptions encountered in the job that was run.
</p>
<p>
    For advice on dealing with exceptions, please see the EDSL <a href="https://docs.expectedparrot.com/en/latest/exceptions.html">documentation</a> page. <br>
    You can also post a question at the Expected Parrot <a href="https://discord.com/invite/mxAYkjfy9m">Discord channel</a>, open an issue on <a href="https://github.com/expectedparrot/edsl">GitHub</a>, or send an email to <a href="mailto:info@expectedparrot.com">info@expectedparrot.com</a>.
</p>

<h2>Overview</h2>
<table border="1">
    <tbody>
        <tr>
            <td>Total interviews</td>
            <td>1</td>
        </tr>
        <tr>
            <td>Interviews with exceptions</td>
            <td>1</td>
        </tr>
    </tbody>
</table>
<p>
    An "interview" is the result of one survey, taken by one agent, with one model and one scenario (if any).
</p>
    <style>
    th, td {
        padding: 0 10px; /* This applies the padding uniformly to all td elements */
    }
</style>

<table border="1">
    <thead>
        <tr>
            <th>Exception Type</th>
            <th>Service</th>
            <th>Model</th>
            <th>Question Name</th>
            <th>Total</th>
        </tr>
    </thead>
    <tbody>
        
            <tr>
                <td>ValueError</td>
                <td>google</td>
                <td>gemini-1.5-flash</td>
                <td>llm_program_estimate</td>
                <td>5</td>
            </tr>
        
            <tr>
                <td>LanguageModelNoResponseError</td>
                <td>google</td>
                <td>gemini-1.5-flash</td>
                <td>llm_program_estimate</td>
                <td>1</td>
            </tr>
        
    </tbody>
</table>
<p>
    <i>Note:</i> You may encounter repeated exceptions where retries were attempted. 
    You can modify the maximum number of attempts for failed API calls in `edsl/config.py`. 
</p>
<p>
    Click to expand the details below for information about each exception, including code for reproducing it.
</p>
    <h2>Exceptions Details</h2>

    
        <style>
    td {
        padding: 0 10px; /* This applies the padding uniformly to all td elements */
    }
    .toggle-btn {
        background-color: #4CAF50;
        color: white;
        border: none;
        padding: 10px 20px;
        text-align: center;
        text-decoration: none;
        display: inline-block;
        font-size: 16px;
        margin: 4px 2px;
        cursor: pointer;
        border-radius: 8px;
        white-space: nowrap;
    }
    .toggle-btn span.collapse {
        display: none;
    }
    .exception-content {
        max-width: 100%; /* Adjust this value based on your layout */
        overflow-x: auto; /* Enables horizontal scrolling */
    }
</style>

<div class="question">question_name: llm_program_estimate</div>


<div class="exception-detail">
    <div class="exception-header">
        <span class="exception-exception">Exception: ValueError("No key found for service 'google'")</span>
        <button id="toggleBtn" class="toggle-btn" onclick="toggleButton(this)" aria-expanded="false">
            <span class="expand"> ▼ </span>
        </button>             
    </div>
    <div class="exception-content">
        <table border="1">
            <tr>
                <td>Interview ID (index in results)</td>
                <td>0</td>
            </tr>
            <tr>
                <td>Question name</td>
                <td>llm_program_estimate</td>
            </tr>
            <tr>
                <td>Question type</td>
                <td>free_text</td>
            </tr>
            <tr>
                <td>Human-readable question</td>
                <td>
        <div id="llm_program_estimate" class="survey_question" data-type="free_text">
            
            <p class="question_text">This household has 1 adult(s) and 0 child(ren). They live in TX. We want to estimate 'eitc' in tax year 2025. Provide a single numeric answer (no extra text).</p>
            
        <div>
        <textarea id="llm_program_estimate" name="llm_program_estimate"></textarea>
        </div>
        
        </div>
        </td>
            </tr>
            <tr>
                <td>User Prompt</td>
                <td><pre>This household has 1 adult(s) and 0 child(ren). They live in TX. We want to estimate 'eitc' in tax year 2025. Provide a single numeric answer (no extra text).</pre></td>
            </tr>
            <tr>
                <td>Scenario</td>
                <td>Scenario({'scenario_index': 0})</td>
            </tr>
            <tr>
                <td>Agent</td>
                <td>Agent(traits = {'persona': 'You are an expert in U.S. tax and benefit laws. Provide only a numeric answer in dollars.'})</td>
            </tr>
            <tr>
                <td>System Prompt</td>
                <td><pre>You are answering questions as if you were a human. Do not break character.Your traits: {'persona': 'You are an expert in U.S. tax and benefit laws. Provide only a numeric answer in dollars.'}</pre></td>
            </tr>
            <tr>
                <td>Inference service</td>
                <td>google</td>
            </tr>
            <tr>
                <td>Model name</td>
                <td>gemini-1.5-flash</td>
            </tr>
            <tr>
                <td>Model parameters</td>
                <td>Model(model_name = 'gemini-1.5-flash', temperature = 1.0, topP = 1, topK = 1, maxOutputTokens = 2048, stopSequences = [])</td>
            </tr>
            <tr>
                <td>Raw model response</td>
                <td><pre>No raw model response available.</pre>
                </td>
            </tr>
            <tr>
                <td>Generated token string (at ['candidates', 0, 'content', 'parts', 0, 'text']) in raw response</td>
                <td><pre>No raw model response available.</pre>
                </td>
            </tr>
            <tr>
            <td>Code likely to reproduce the error</td>
            <td>
                <textarea id="codeToCopy" rows="10" cols="90">from edsl import Question, Model, Scenario, Agent
q = Question('free_text', question_name = """llm_program_estimate""", question_text = """This household has 1 adult(s) and 0 child(ren). They live in TX. We want to estimate 'eitc' in tax year 2025. Provide a single numeric answer (no extra text).""")
scenario = Scenario({})
agent = Agent(traits = {'persona': 'You are an expert in U.S. tax and benefit laws. Provide only a numeric answer in dollars.'})
m = Model('gemini-1.5-flash')
results = q.by(m).by(agent).by(scenario).run()</textarea>
                <button onclick="copyCode()">Copy</button>
            </td>
            </tr>

        </table>
    
        
        <br><br>
        <div class="exception-time">Time: 2025-02-15T20:15:57.690598</div>          
        <div class="exception-traceback">Traceback: 
            <text>
            <pre>Traceback (most recent call last):
  File "/opt/homebrew/lib/python3.10/site-packages/edsl/jobs/AnswerQuestionFunctionConstructor.py", line 171, in attempt_answer
    await invigilator.async_answer_question()
  File "/opt/homebrew/lib/python3.10/site-packages/edsl/agents/Invigilator.py", line 59, in async_answer_question
    agent_response_dict: AgentResponseDict = await self.async_get_agent_response()
  File "/opt/homebrew/lib/python3.10/site-packages/edsl/agents/Invigilator.py", line 46, in async_get_agent_response
    return await self.model.async_get_response(**params)
  File "/opt/homebrew/lib/python3.10/site-packages/edsl/language_models/LanguageModel.py", line 460, in async_get_response
    await self._async_get_intended_model_call_outcome(**params)
  File "/opt/homebrew/lib/python3.10/site-packages/edsl/language_models/LanguageModel.py", line 398, in _async_get_intended_model_call_outcome
    response = await asyncio.wait_for(f(**params), timeout=TIMEOUT)
  File "/opt/homebrew/Cellar/python@3.10/3.10.16/Frameworks/Python.framework/Versions/3.10/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/homebrew/Cellar/python@3.10/3.10.16/Frameworks/Python.framework/Versions/3.10/lib/python3.10/asyncio/futures.py", line 201, in result
    raise self._exception.with_traceback(self._exception_tb)
  File "/opt/homebrew/Cellar/python@3.10/3.10.16/Frameworks/Python.framework/Versions/3.10/lib/python3.10/asyncio/tasks.py", line 232, in __step
    result = coro.send(None)
  File "/opt/homebrew/lib/python3.10/site-packages/edsl/inference_services/GoogleService.py", line 103, in async_execute_model_call
    genai.configure(api_key=self.api_token)
  File "/opt/homebrew/lib/python3.10/site-packages/edsl/language_models/LanguageModel.py", line 206, in api_token
    raise ValueError(
ValueError: No key found for service 'google'
</pre>
            </text>
        </div>
    </div>
</div>


<div class="exception-detail">
    <div class="exception-header">
        <span class="exception-exception">Exception: ValueError("No key found for service 'google'")</span>
        <button id="toggleBtn" class="toggle-btn" onclick="toggleButton(this)" aria-expanded="false">
            <span class="expand"> ▼ </span>
        </button>             
    </div>
    <div class="exception-content">
        <table border="1">
            <tr>
                <td>Interview ID (index in results)</td>
                <td>0</td>
            </tr>
            <tr>
                <td>Question name</td>
                <td>llm_program_estimate</td>
            </tr>
            <tr>
                <td>Question type</td>
                <td>free_text</td>
            </tr>
            <tr>
                <td>Human-readable question</td>
                <td>
        <div id="llm_program_estimate" class="survey_question" data-type="free_text">
            
            <p class="question_text">This household has 1 adult(s) and 0 child(ren). They live in TX. We want to estimate 'eitc' in tax year 2025. Provide a single numeric answer (no extra text).</p>
            
        <div>
        <textarea id="llm_program_estimate" name="llm_program_estimate"></textarea>
        </div>
        
        </div>
        </td>
            </tr>
            <tr>
                <td>User Prompt</td>
                <td><pre>This household has 1 adult(s) and 0 child(ren). They live in TX. We want to estimate 'eitc' in tax year 2025. Provide a single numeric answer (no extra text).</pre></td>
            </tr>
            <tr>
                <td>Scenario</td>
                <td>Scenario({'scenario_index': 0})</td>
            </tr>
            <tr>
                <td>Agent</td>
                <td>Agent(traits = {'persona': 'You are an expert in U.S. tax and benefit laws. Provide only a numeric answer in dollars.'})</td>
            </tr>
            <tr>
                <td>System Prompt</td>
                <td><pre>You are answering questions as if you were a human. Do not break character.Your traits: {'persona': 'You are an expert in U.S. tax and benefit laws. Provide only a numeric answer in dollars.'}</pre></td>
            </tr>
            <tr>
                <td>Inference service</td>
                <td>google</td>
            </tr>
            <tr>
                <td>Model name</td>
                <td>gemini-1.5-flash</td>
            </tr>
            <tr>
                <td>Model parameters</td>
                <td>Model(model_name = 'gemini-1.5-flash', temperature = 1.0, topP = 1, topK = 1, maxOutputTokens = 2048, stopSequences = [])</td>
            </tr>
            <tr>
                <td>Raw model response</td>
                <td><pre>No raw model response available.</pre>
                </td>
            </tr>
            <tr>
                <td>Generated token string (at ['candidates', 0, 'content', 'parts', 0, 'text']) in raw response</td>
                <td><pre>No raw model response available.</pre>
                </td>
            </tr>
            <tr>
            <td>Code likely to reproduce the error</td>
            <td>
                <textarea id="codeToCopy" rows="10" cols="90">from edsl import Question, Model, Scenario, Agent
q = Question('free_text', question_name = """llm_program_estimate""", question_text = """This household has 1 adult(s) and 0 child(ren). They live in TX. We want to estimate 'eitc' in tax year 2025. Provide a single numeric answer (no extra text).""")
scenario = Scenario({})
agent = Agent(traits = {'persona': 'You are an expert in U.S. tax and benefit laws. Provide only a numeric answer in dollars.'})
m = Model('gemini-1.5-flash')
results = q.by(m).by(agent).by(scenario).run()</textarea>
                <button onclick="copyCode()">Copy</button>
            </td>
            </tr>

        </table>
    
        
        <br><br>
        <div class="exception-time">Time: 2025-02-15T20:15:58.699989</div>          
        <div class="exception-traceback">Traceback: 
            <text>
            <pre>Traceback (most recent call last):
  File "/opt/homebrew/lib/python3.10/site-packages/edsl/jobs/AnswerQuestionFunctionConstructor.py", line 171, in attempt_answer
    await invigilator.async_answer_question()
  File "/opt/homebrew/lib/python3.10/site-packages/edsl/agents/Invigilator.py", line 59, in async_answer_question
    agent_response_dict: AgentResponseDict = await self.async_get_agent_response()
  File "/opt/homebrew/lib/python3.10/site-packages/edsl/agents/Invigilator.py", line 46, in async_get_agent_response
    return await self.model.async_get_response(**params)
  File "/opt/homebrew/lib/python3.10/site-packages/edsl/language_models/LanguageModel.py", line 460, in async_get_response
    await self._async_get_intended_model_call_outcome(**params)
  File "/opt/homebrew/lib/python3.10/site-packages/edsl/language_models/LanguageModel.py", line 398, in _async_get_intended_model_call_outcome
    response = await asyncio.wait_for(f(**params), timeout=TIMEOUT)
  File "/opt/homebrew/Cellar/python@3.10/3.10.16/Frameworks/Python.framework/Versions/3.10/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/homebrew/Cellar/python@3.10/3.10.16/Frameworks/Python.framework/Versions/3.10/lib/python3.10/asyncio/futures.py", line 201, in result
    raise self._exception.with_traceback(self._exception_tb)
  File "/opt/homebrew/Cellar/python@3.10/3.10.16/Frameworks/Python.framework/Versions/3.10/lib/python3.10/asyncio/tasks.py", line 232, in __step
    result = coro.send(None)
  File "/opt/homebrew/lib/python3.10/site-packages/edsl/inference_services/GoogleService.py", line 103, in async_execute_model_call
    genai.configure(api_key=self.api_token)
  File "/opt/homebrew/lib/python3.10/site-packages/edsl/language_models/LanguageModel.py", line 206, in api_token
    raise ValueError(
ValueError: No key found for service 'google'
</pre>
            </text>
        </div>
    </div>
</div>


<div class="exception-detail">
    <div class="exception-header">
        <span class="exception-exception">Exception: ValueError("No key found for service 'google'")</span>
        <button id="toggleBtn" class="toggle-btn" onclick="toggleButton(this)" aria-expanded="false">
            <span class="expand"> ▼ </span>
        </button>             
    </div>
    <div class="exception-content">
        <table border="1">
            <tr>
                <td>Interview ID (index in results)</td>
                <td>0</td>
            </tr>
            <tr>
                <td>Question name</td>
                <td>llm_program_estimate</td>
            </tr>
            <tr>
                <td>Question type</td>
                <td>free_text</td>
            </tr>
            <tr>
                <td>Human-readable question</td>
                <td>
        <div id="llm_program_estimate" class="survey_question" data-type="free_text">
            
            <p class="question_text">This household has 1 adult(s) and 0 child(ren). They live in TX. We want to estimate 'eitc' in tax year 2025. Provide a single numeric answer (no extra text).</p>
            
        <div>
        <textarea id="llm_program_estimate" name="llm_program_estimate"></textarea>
        </div>
        
        </div>
        </td>
            </tr>
            <tr>
                <td>User Prompt</td>
                <td><pre>This household has 1 adult(s) and 0 child(ren). They live in TX. We want to estimate 'eitc' in tax year 2025. Provide a single numeric answer (no extra text).</pre></td>
            </tr>
            <tr>
                <td>Scenario</td>
                <td>Scenario({'scenario_index': 0})</td>
            </tr>
            <tr>
                <td>Agent</td>
                <td>Agent(traits = {'persona': 'You are an expert in U.S. tax and benefit laws. Provide only a numeric answer in dollars.'})</td>
            </tr>
            <tr>
                <td>System Prompt</td>
                <td><pre>You are answering questions as if you were a human. Do not break character.Your traits: {'persona': 'You are an expert in U.S. tax and benefit laws. Provide only a numeric answer in dollars.'}</pre></td>
            </tr>
            <tr>
                <td>Inference service</td>
                <td>google</td>
            </tr>
            <tr>
                <td>Model name</td>
                <td>gemini-1.5-flash</td>
            </tr>
            <tr>
                <td>Model parameters</td>
                <td>Model(model_name = 'gemini-1.5-flash', temperature = 1.0, topP = 1, topK = 1, maxOutputTokens = 2048, stopSequences = [])</td>
            </tr>
            <tr>
                <td>Raw model response</td>
                <td><pre>No raw model response available.</pre>
                </td>
            </tr>
            <tr>
                <td>Generated token string (at ['candidates', 0, 'content', 'parts', 0, 'text']) in raw response</td>
                <td><pre>No raw model response available.</pre>
                </td>
            </tr>
            <tr>
            <td>Code likely to reproduce the error</td>
            <td>
                <textarea id="codeToCopy" rows="10" cols="90">from edsl import Question, Model, Scenario, Agent
q = Question('free_text', question_name = """llm_program_estimate""", question_text = """This household has 1 adult(s) and 0 child(ren). They live in TX. We want to estimate 'eitc' in tax year 2025. Provide a single numeric answer (no extra text).""")
scenario = Scenario({})
agent = Agent(traits = {'persona': 'You are an expert in U.S. tax and benefit laws. Provide only a numeric answer in dollars.'})
m = Model('gemini-1.5-flash')
results = q.by(m).by(agent).by(scenario).run()</textarea>
                <button onclick="copyCode()">Copy</button>
            </td>
            </tr>

        </table>
    
        
        <br><br>
        <div class="exception-time">Time: 2025-02-15T20:16:00.703584</div>          
        <div class="exception-traceback">Traceback: 
            <text>
            <pre>Traceback (most recent call last):
  File "/opt/homebrew/lib/python3.10/site-packages/edsl/jobs/AnswerQuestionFunctionConstructor.py", line 171, in attempt_answer
    await invigilator.async_answer_question()
  File "/opt/homebrew/lib/python3.10/site-packages/edsl/agents/Invigilator.py", line 59, in async_answer_question
    agent_response_dict: AgentResponseDict = await self.async_get_agent_response()
  File "/opt/homebrew/lib/python3.10/site-packages/edsl/agents/Invigilator.py", line 46, in async_get_agent_response
    return await self.model.async_get_response(**params)
  File "/opt/homebrew/lib/python3.10/site-packages/edsl/language_models/LanguageModel.py", line 460, in async_get_response
    await self._async_get_intended_model_call_outcome(**params)
  File "/opt/homebrew/lib/python3.10/site-packages/edsl/language_models/LanguageModel.py", line 398, in _async_get_intended_model_call_outcome
    response = await asyncio.wait_for(f(**params), timeout=TIMEOUT)
  File "/opt/homebrew/Cellar/python@3.10/3.10.16/Frameworks/Python.framework/Versions/3.10/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/homebrew/Cellar/python@3.10/3.10.16/Frameworks/Python.framework/Versions/3.10/lib/python3.10/asyncio/futures.py", line 201, in result
    raise self._exception.with_traceback(self._exception_tb)
  File "/opt/homebrew/Cellar/python@3.10/3.10.16/Frameworks/Python.framework/Versions/3.10/lib/python3.10/asyncio/tasks.py", line 232, in __step
    result = coro.send(None)
  File "/opt/homebrew/lib/python3.10/site-packages/edsl/inference_services/GoogleService.py", line 103, in async_execute_model_call
    genai.configure(api_key=self.api_token)
  File "/opt/homebrew/lib/python3.10/site-packages/edsl/language_models/LanguageModel.py", line 206, in api_token
    raise ValueError(
ValueError: No key found for service 'google'
</pre>
            </text>
        </div>
    </div>
</div>


<div class="exception-detail">
    <div class="exception-header">
        <span class="exception-exception">Exception: ValueError("No key found for service 'google'")</span>
        <button id="toggleBtn" class="toggle-btn" onclick="toggleButton(this)" aria-expanded="false">
            <span class="expand"> ▼ </span>
        </button>             
    </div>
    <div class="exception-content">
        <table border="1">
            <tr>
                <td>Interview ID (index in results)</td>
                <td>0</td>
            </tr>
            <tr>
                <td>Question name</td>
                <td>llm_program_estimate</td>
            </tr>
            <tr>
                <td>Question type</td>
                <td>free_text</td>
            </tr>
            <tr>
                <td>Human-readable question</td>
                <td>
        <div id="llm_program_estimate" class="survey_question" data-type="free_text">
            
            <p class="question_text">This household has 1 adult(s) and 0 child(ren). They live in TX. We want to estimate 'eitc' in tax year 2025. Provide a single numeric answer (no extra text).</p>
            
        <div>
        <textarea id="llm_program_estimate" name="llm_program_estimate"></textarea>
        </div>
        
        </div>
        </td>
            </tr>
            <tr>
                <td>User Prompt</td>
                <td><pre>This household has 1 adult(s) and 0 child(ren). They live in TX. We want to estimate 'eitc' in tax year 2025. Provide a single numeric answer (no extra text).</pre></td>
            </tr>
            <tr>
                <td>Scenario</td>
                <td>Scenario({'scenario_index': 0})</td>
            </tr>
            <tr>
                <td>Agent</td>
                <td>Agent(traits = {'persona': 'You are an expert in U.S. tax and benefit laws. Provide only a numeric answer in dollars.'})</td>
            </tr>
            <tr>
                <td>System Prompt</td>
                <td><pre>You are answering questions as if you were a human. Do not break character.Your traits: {'persona': 'You are an expert in U.S. tax and benefit laws. Provide only a numeric answer in dollars.'}</pre></td>
            </tr>
            <tr>
                <td>Inference service</td>
                <td>google</td>
            </tr>
            <tr>
                <td>Model name</td>
                <td>gemini-1.5-flash</td>
            </tr>
            <tr>
                <td>Model parameters</td>
                <td>Model(model_name = 'gemini-1.5-flash', temperature = 1.0, topP = 1, topK = 1, maxOutputTokens = 2048, stopSequences = [])</td>
            </tr>
            <tr>
                <td>Raw model response</td>
                <td><pre>No raw model response available.</pre>
                </td>
            </tr>
            <tr>
                <td>Generated token string (at ['candidates', 0, 'content', 'parts', 0, 'text']) in raw response</td>
                <td><pre>No raw model response available.</pre>
                </td>
            </tr>
            <tr>
            <td>Code likely to reproduce the error</td>
            <td>
                <textarea id="codeToCopy" rows="10" cols="90">from edsl import Question, Model, Scenario, Agent
q = Question('free_text', question_name = """llm_program_estimate""", question_text = """This household has 1 adult(s) and 0 child(ren). They live in TX. We want to estimate 'eitc' in tax year 2025. Provide a single numeric answer (no extra text).""")
scenario = Scenario({})
agent = Agent(traits = {'persona': 'You are an expert in U.S. tax and benefit laws. Provide only a numeric answer in dollars.'})
m = Model('gemini-1.5-flash')
results = q.by(m).by(agent).by(scenario).run()</textarea>
                <button onclick="copyCode()">Copy</button>
            </td>
            </tr>

        </table>
    
        
        <br><br>
        <div class="exception-time">Time: 2025-02-15T20:16:04.711126</div>          
        <div class="exception-traceback">Traceback: 
            <text>
            <pre>Traceback (most recent call last):
  File "/opt/homebrew/lib/python3.10/site-packages/edsl/jobs/AnswerQuestionFunctionConstructor.py", line 171, in attempt_answer
    await invigilator.async_answer_question()
  File "/opt/homebrew/lib/python3.10/site-packages/edsl/agents/Invigilator.py", line 59, in async_answer_question
    agent_response_dict: AgentResponseDict = await self.async_get_agent_response()
  File "/opt/homebrew/lib/python3.10/site-packages/edsl/agents/Invigilator.py", line 46, in async_get_agent_response
    return await self.model.async_get_response(**params)
  File "/opt/homebrew/lib/python3.10/site-packages/edsl/language_models/LanguageModel.py", line 460, in async_get_response
    await self._async_get_intended_model_call_outcome(**params)
  File "/opt/homebrew/lib/python3.10/site-packages/edsl/language_models/LanguageModel.py", line 398, in _async_get_intended_model_call_outcome
    response = await asyncio.wait_for(f(**params), timeout=TIMEOUT)
  File "/opt/homebrew/Cellar/python@3.10/3.10.16/Frameworks/Python.framework/Versions/3.10/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/homebrew/Cellar/python@3.10/3.10.16/Frameworks/Python.framework/Versions/3.10/lib/python3.10/asyncio/futures.py", line 201, in result
    raise self._exception.with_traceback(self._exception_tb)
  File "/opt/homebrew/Cellar/python@3.10/3.10.16/Frameworks/Python.framework/Versions/3.10/lib/python3.10/asyncio/tasks.py", line 232, in __step
    result = coro.send(None)
  File "/opt/homebrew/lib/python3.10/site-packages/edsl/inference_services/GoogleService.py", line 103, in async_execute_model_call
    genai.configure(api_key=self.api_token)
  File "/opt/homebrew/lib/python3.10/site-packages/edsl/language_models/LanguageModel.py", line 206, in api_token
    raise ValueError(
ValueError: No key found for service 'google'
</pre>
            </text>
        </div>
    </div>
</div>


<div class="exception-detail">
    <div class="exception-header">
        <span class="exception-exception">Exception: ValueError("No key found for service 'google'")</span>
        <button id="toggleBtn" class="toggle-btn" onclick="toggleButton(this)" aria-expanded="false">
            <span class="expand"> ▼ </span>
        </button>             
    </div>
    <div class="exception-content">
        <table border="1">
            <tr>
                <td>Interview ID (index in results)</td>
                <td>0</td>
            </tr>
            <tr>
                <td>Question name</td>
                <td>llm_program_estimate</td>
            </tr>
            <tr>
                <td>Question type</td>
                <td>free_text</td>
            </tr>
            <tr>
                <td>Human-readable question</td>
                <td>
        <div id="llm_program_estimate" class="survey_question" data-type="free_text">
            
            <p class="question_text">This household has 1 adult(s) and 0 child(ren). They live in TX. We want to estimate 'eitc' in tax year 2025. Provide a single numeric answer (no extra text).</p>
            
        <div>
        <textarea id="llm_program_estimate" name="llm_program_estimate"></textarea>
        </div>
        
        </div>
        </td>
            </tr>
            <tr>
                <td>User Prompt</td>
                <td><pre>This household has 1 adult(s) and 0 child(ren). They live in TX. We want to estimate 'eitc' in tax year 2025. Provide a single numeric answer (no extra text).</pre></td>
            </tr>
            <tr>
                <td>Scenario</td>
                <td>Scenario({'scenario_index': 0})</td>
            </tr>
            <tr>
                <td>Agent</td>
                <td>Agent(traits = {'persona': 'You are an expert in U.S. tax and benefit laws. Provide only a numeric answer in dollars.'})</td>
            </tr>
            <tr>
                <td>System Prompt</td>
                <td><pre>You are answering questions as if you were a human. Do not break character.Your traits: {'persona': 'You are an expert in U.S. tax and benefit laws. Provide only a numeric answer in dollars.'}</pre></td>
            </tr>
            <tr>
                <td>Inference service</td>
                <td>google</td>
            </tr>
            <tr>
                <td>Model name</td>
                <td>gemini-1.5-flash</td>
            </tr>
            <tr>
                <td>Model parameters</td>
                <td>Model(model_name = 'gemini-1.5-flash', temperature = 1.0, topP = 1, topK = 1, maxOutputTokens = 2048, stopSequences = [])</td>
            </tr>
            <tr>
                <td>Raw model response</td>
                <td><pre>No raw model response available.</pre>
                </td>
            </tr>
            <tr>
                <td>Generated token string (at ['candidates', 0, 'content', 'parts', 0, 'text']) in raw response</td>
                <td><pre>No raw model response available.</pre>
                </td>
            </tr>
            <tr>
            <td>Code likely to reproduce the error</td>
            <td>
                <textarea id="codeToCopy" rows="10" cols="90">from edsl import Question, Model, Scenario, Agent
q = Question('free_text', question_name = """llm_program_estimate""", question_text = """This household has 1 adult(s) and 0 child(ren). They live in TX. We want to estimate 'eitc' in tax year 2025. Provide a single numeric answer (no extra text).""")
scenario = Scenario({})
agent = Agent(traits = {'persona': 'You are an expert in U.S. tax and benefit laws. Provide only a numeric answer in dollars.'})
m = Model('gemini-1.5-flash')
results = q.by(m).by(agent).by(scenario).run()</textarea>
                <button onclick="copyCode()">Copy</button>
            </td>
            </tr>

        </table>
    
        
        <br><br>
        <div class="exception-time">Time: 2025-02-15T20:16:12.745271</div>          
        <div class="exception-traceback">Traceback: 
            <text>
            <pre>Traceback (most recent call last):
  File "/opt/homebrew/lib/python3.10/site-packages/edsl/jobs/AnswerQuestionFunctionConstructor.py", line 171, in attempt_answer
    await invigilator.async_answer_question()
  File "/opt/homebrew/lib/python3.10/site-packages/edsl/agents/Invigilator.py", line 59, in async_answer_question
    agent_response_dict: AgentResponseDict = await self.async_get_agent_response()
  File "/opt/homebrew/lib/python3.10/site-packages/edsl/agents/Invigilator.py", line 46, in async_get_agent_response
    return await self.model.async_get_response(**params)
  File "/opt/homebrew/lib/python3.10/site-packages/edsl/language_models/LanguageModel.py", line 460, in async_get_response
    await self._async_get_intended_model_call_outcome(**params)
  File "/opt/homebrew/lib/python3.10/site-packages/edsl/language_models/LanguageModel.py", line 398, in _async_get_intended_model_call_outcome
    response = await asyncio.wait_for(f(**params), timeout=TIMEOUT)
  File "/opt/homebrew/Cellar/python@3.10/3.10.16/Frameworks/Python.framework/Versions/3.10/lib/python3.10/asyncio/tasks.py", line 445, in wait_for
    return fut.result()
  File "/opt/homebrew/Cellar/python@3.10/3.10.16/Frameworks/Python.framework/Versions/3.10/lib/python3.10/asyncio/futures.py", line 201, in result
    raise self._exception.with_traceback(self._exception_tb)
  File "/opt/homebrew/Cellar/python@3.10/3.10.16/Frameworks/Python.framework/Versions/3.10/lib/python3.10/asyncio/tasks.py", line 232, in __step
    result = coro.send(None)
  File "/opt/homebrew/lib/python3.10/site-packages/edsl/inference_services/GoogleService.py", line 103, in async_execute_model_call
    genai.configure(api_key=self.api_token)
  File "/opt/homebrew/lib/python3.10/site-packages/edsl/language_models/LanguageModel.py", line 206, in api_token
    raise ValueError(
ValueError: No key found for service 'google'
</pre>
            </text>
        </div>
    </div>
</div>


<div class="exception-detail">
    <div class="exception-header">
        <span class="exception-exception">Exception: LanguageModelNoResponseError("Language model did not return a response for question 'llm_program_estimate.'")</span>
        <button id="toggleBtn" class="toggle-btn" onclick="toggleButton(this)" aria-expanded="false">
            <span class="expand"> ▼ </span>
        </button>             
    </div>
    <div class="exception-content">
        <table border="1">
            <tr>
                <td>Interview ID (index in results)</td>
                <td>0</td>
            </tr>
            <tr>
                <td>Question name</td>
                <td>llm_program_estimate</td>
            </tr>
            <tr>
                <td>Question type</td>
                <td>free_text</td>
            </tr>
            <tr>
                <td>Human-readable question</td>
                <td>
        <div id="llm_program_estimate" class="survey_question" data-type="free_text">
            
            <p class="question_text">This household has 1 adult(s) and 0 child(ren). They live in TX. We want to estimate 'eitc' in tax year 2025. Provide a single numeric answer (no extra text).</p>
            
        <div>
        <textarea id="llm_program_estimate" name="llm_program_estimate"></textarea>
        </div>
        
        </div>
        </td>
            </tr>
            <tr>
                <td>User Prompt</td>
                <td><pre>This household has 1 adult(s) and 0 child(ren). They live in TX. We want to estimate 'eitc' in tax year 2025. Provide a single numeric answer (no extra text).</pre></td>
            </tr>
            <tr>
                <td>Scenario</td>
                <td>Scenario({'scenario_index': 0})</td>
            </tr>
            <tr>
                <td>Agent</td>
                <td>Agent(traits = {'persona': 'You are an expert in U.S. tax and benefit laws. Provide only a numeric answer in dollars.'})</td>
            </tr>
            <tr>
                <td>System Prompt</td>
                <td><pre>You are answering questions as if you were a human. Do not break character.Your traits: {'persona': 'You are an expert in U.S. tax and benefit laws. Provide only a numeric answer in dollars.'}</pre></td>
            </tr>
            <tr>
                <td>Inference service</td>
                <td>google</td>
            </tr>
            <tr>
                <td>Model name</td>
                <td>gemini-1.5-flash</td>
            </tr>
            <tr>
                <td>Model parameters</td>
                <td>Model(model_name = 'gemini-1.5-flash', temperature = 1.0, topP = 1, topK = 1, maxOutputTokens = 2048, stopSequences = [])</td>
            </tr>
            <tr>
                <td>Raw model response</td>
                <td><pre>No raw model response available.</pre>
                </td>
            </tr>
            <tr>
                <td>Generated token string (at ['candidates', 0, 'content', 'parts', 0, 'text']) in raw response</td>
                <td><pre>No raw model response available.</pre>
                </td>
            </tr>
            <tr>
            <td>Code likely to reproduce the error</td>
            <td>
                <textarea id="codeToCopy" rows="10" cols="90">from edsl import Question, Model, Scenario, Agent
q = Question('free_text', question_name = """llm_program_estimate""", question_text = """This household has 1 adult(s) and 0 child(ren). They live in TX. We want to estimate 'eitc' in tax year 2025. Provide a single numeric answer (no extra text).""")
scenario = Scenario({})
agent = Agent(traits = {'persona': 'You are an expert in U.S. tax and benefit laws. Provide only a numeric answer in dollars.'})
m = Model('gemini-1.5-flash')
results = q.by(m).by(agent).by(scenario).run()</textarea>
                <button onclick="copyCode()">Copy</button>
            </td>
            </tr>

        </table>
    
        
        <br><br>
        <div class="exception-time">Time: 2025-02-15T20:16:12.746175</div>          
        <div class="exception-traceback">Traceback: 
            <text>
            <pre>Traceback (most recent call last):
  File "/opt/homebrew/lib/python3.10/site-packages/edsl/jobs/interviews/Interview.py", line 312, in handle_task
    result: Answers = task.result()
  File "/opt/homebrew/Cellar/python@3.10/3.10.16/Frameworks/Python.framework/Versions/3.10/lib/python3.10/asyncio/futures.py", line 201, in result
    raise self._exception.with_traceback(self._exception_tb)
  File "/opt/homebrew/Cellar/python@3.10/3.10.16/Frameworks/Python.framework/Versions/3.10/lib/python3.10/asyncio/tasks.py", line 232, in __step
    result = coro.send(None)
  File "/opt/homebrew/lib/python3.10/site-packages/edsl/jobs/tasks/QuestionTaskCreator.py", line 238, in _run_task_async
    return await self._run_focal_task()
  File "/opt/homebrew/lib/python3.10/site-packages/edsl/jobs/tasks/QuestionTaskCreator.py", line 144, in _run_focal_task
    raise e
  File "/opt/homebrew/lib/python3.10/site-packages/edsl/jobs/tasks/QuestionTaskCreator.py", line 138, in _run_focal_task
    results = await self.answer_question_func(
  File "/opt/homebrew/lib/python3.10/site-packages/edsl/jobs/AnswerQuestionFunctionConstructor.py", line 217, in answer_question_and_record_task
    return await attempt_answer()
  File "/opt/homebrew/lib/python3.10/site-packages/tenacity/_asyncio.py", line 88, in async_wrapped
    return await fn(*args, **kwargs)
  File "/opt/homebrew/lib/python3.10/site-packages/tenacity/_asyncio.py", line 47, in __call__
    do = self.iter(retry_state=retry_state)
  File "/opt/homebrew/lib/python3.10/site-packages/tenacity/__init__.py", line 325, in iter
    raise retry_exc.reraise()
  File "/opt/homebrew/lib/python3.10/site-packages/tenacity/__init__.py", line 158, in reraise
    raise self.last_attempt.result()
  File "/opt/homebrew/Cellar/python@3.10/3.10.16/Frameworks/Python.framework/Versions/3.10/lib/python3.10/concurrent/futures/_base.py", line 451, in result
    return self.__get_result()
  File "/opt/homebrew/Cellar/python@3.10/3.10.16/Frameworks/Python.framework/Versions/3.10/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/opt/homebrew/lib/python3.10/site-packages/tenacity/_asyncio.py", line 50, in __call__
    result = await fn(*args, **kwargs)
  File "/opt/homebrew/lib/python3.10/site-packages/edsl/jobs/AnswerQuestionFunctionConstructor.py", line 204, in attempt_answer
    raise LanguageModelNoResponseError(
edsl.exceptions.language_models.LanguageModelNoResponseError: Language model did not return a response for question 'llm_program_estimate.'
</pre>
            </text>
        </div>
    </div>
</div>


    

    <h2>Performance Plot</h2>

</body>
</html>